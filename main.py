from datetime import date, timedelta
import datetime
from pprint import pprint
import json
import requests as r
import time
from typing import List, Dict

def make_date_range(start_date:datetime.timedelta, end_date: datetime.timedelta) -> datetime.timedelta:
    delta = end_date - start_date
    timestamps = []

    for i in range(delta.days + 1):
        day = start_date + timedelta(days=i)
        unix_timestamp = str(int(time.mktime(day.timetuple())))
        timestamps.append(unix_timestamp)

    return timestamps

def export_submissions(subreddit:str, query:str, timestamps:datetime.timedelta, num_retries:int=1, file_export_name:str="data2") -> List[str]:
    ''' 
    Exports query as a JSON object and retrieves a list of ids for further processing

    '''

    for i in range(len(timestamps)):
        
        for num_retry in range(num_retries+1):

            try:

                if i == len(timestamps)-1:
                    break

                after = timestamps[i]
                before = timestamps[i+1]
                api_call = f'https://api.pushshift.io/reddit/search/submission/?q={query}&subreddit={subreddit}&after={after}&before={before}&limit=500'
                
                time.sleep(1.2)

                req = r.get(api_call)

                submission = req.json()['data'] 

                with open(f"{file_export_name}.json", 'a', encoding='utf-8') as f:
                    json.dump(submission, f, ensure_ascii=False, indent=4)
                
                print(f"Data exported to {file_export_name}.csv")

            except json.decoder.JSONDecodeError:
                print(f"Failed to parse: {api_call}")
                print(f"Adding url to {file_export_name}_errors.txt file") 

                with open(f"{file_export_name}_error_url.txt", 'a', encoding='utf-8') as error_file:
                    error_file.write(f"{api_call}\n")
                
                print(f"Done adding url to {file_export_name}_errors.txt file.  Continuing on...") 
                continue 

            break

        return [data['id'] for data in submission]

def get_comments_id_from_submission_id(submission_id:str, file_export_name="data_comment_and_sub_ids2") -> List[str]:
    '''
    Given the ids generated by the submission endpoint, their corresponding 
    comment ids are generated. 

    Return:
        List[Dict[str,str]] 
        
        A list of dictionaries containing the comment id and the submission id
    '''
    list_comments_submission_id = list()

    comment_ids = f"https://api.pushshift.io/reddit/submission/comment_ids/{submission_id}"
    comment_ids = r.get(comment_ids).json()["data"]    

    for id in comment_ids: 
        list_comments_submission_id.append({"submission_id":submission_id, "comment_id": id})
    
    with open(f"{file_export_name}.json", 'a', encoding='utf-8') as f:
        json.dump(list_comments_submission_id, f, ensure_ascii=False, indent=4)
            
    print(f"Data exported to {file_export_name}.csv")

    return[item["comment_id"] for item in list_comments_submission_id]


def get_comments(comment_ids:List[str], num_retries:int=1, file_export_name:str="comments_data2") -> None:
    
    comment_string = ','.join(comment_ids)

    for num_retry in range(num_retries+1):

        try:

            api_call = f'https://api.pushshift.io/reddit/search/comment?ids={comment_string}'
            
            time.sleep(1.2)

            req = r.get(api_call)

            submission = req.json()['data'] 

            with open(f"{file_export_name}.json", 'a', encoding='utf-8') as f:
                json.dump(submission, f, ensure_ascii=False, indent=4)
            
            print(f"Data exported to {file_export_name}.csv")

        except json.decoder.JSONDecodeError:
            print(f"Failed to parse: {api_call}")
            print(f"Adding url to {file_export_name}_errors.txt file") 

            with open(f"{file_export_name}_error_url.txt", 'a', encoding='utf-8') as error_file:
                error_file.write(f"{api_call}\n")
            
            print(f"Done adding url to {file_export_name}_errors.txt file.  Continuing on...") 
            continue 

        break


def main() -> None:
    #edate = date.today()
    start_date = date(2020, 4, 1)
    end_date = date(2020, 4, 5)
    timestamps = make_date_range(start_date, end_date)
    
    queries=['covid', 'coronavirus','sars-cov-2']
    subreddits=['Canada', 'CanadaPolitics', 'CanadaCoronavirus', 
                'Vancouver', 'Edmonton', 'Winnipeg', 
                'Montreal', 'Ottawa', 'Saskatoon', 
                'Calgary', 'Toronto', 'Ontario', "onguardforthee"]
    
    for query in queries:
        print(f'Query: {query}')

        for subreddit in subreddits:
            print(f'Subreddit: {subreddit}')

            posts_id = export_submissions(subreddit=subreddit, query=query, timestamps=timestamps) 

            for post_id in posts_id:
                comments_id = get_comments_id_from_submission_id(post_id)

                get_comments(comments_id)


if __name__ == "__main__":
    main()