from datetime import date, timedelta
import datetime
import json
import requests as r
import time
from typing import List, Dict
from export_to_json import build_json_file, build_error_text_file

def make_date_range(start_date:datetime.timedelta, end_date: datetime.timedelta) -> datetime.timedelta:
    delta = end_date - start_date
    timestamps = []

    for i in range(delta.days + 1):
        day = start_date + timedelta(days=i)
        unix_timestamp = str(int(time.mktime(day.timetuple())))
        timestamps.append(unix_timestamp)

    return timestamps

def export_submissions(subreddit:str, query:str, timestamps:datetime.timedelta, num_retries:int=1, file_export_name:str="data2") -> List[str]:
    '''
    Exports query as a JSON object and retrieves a list of ids for further processing

    Return:
        List[str]
            
        All the IDs retrieved by the API
    '''

    for i in range(len(timestamps)):
        
        for num_retry in range(num_retries+1):

            try:

                if i == len(timestamps)-1:
                    break

                after = timestamps[i]
                before = timestamps[i+1]
                api_call = f'https://api.pushshift.io/reddit/search/submission/?q={query}&subreddit={subreddit}&after={after}&before={before}&limit=500'
                
                time.sleep(2)

                req = r.get(api_call)

                submission = req.json()['data'] 

                build_json_file(submission, file_export_name)

                print(f"Data exported to {file_export_name}.csv")

            except json.decoder.JSONDecodeError:
                print(f"Failed to parse: {api_call}")
                print(f"Adding to an error.txt file") 

                build_error_text_file(f"api_call\n", file_export_name)

                print(f"Done adding url to {file_export_name}_errors.txt file.  Continuing on...") 
                continue 

            break

        return [data['id'] for data in submission]

def get_comments_id_from_submission_id(submission_id:str, num_retries:int=1, file_export_name="data_comment_and_sub_ids2") -> List[str]:
    '''
    Given the ids generated by the submission endpoint, their corresponding 
    comment ids are generated. 

    Return:
        List[Dict[str,str]] 
        
        A list of dictionaries containing the comment id and the submission id
    '''
    list_comments_submission_id = list()

    for num_retry in range(num_retries+1):

        try:

            comment_ids = f"https://api.pushshift.io/reddit/submission/comment_ids/{submission_id}"
            comment_ids = r.get(comment_ids).json()["data"]    

            time.sleep(2)

            for id in comment_ids: 
                list_comments_submission_id.append({"submission_id":submission_id, "comment_id": id})
            
            build_json_file(list_comments_submission_id, file_export_name)
        
            print(f"Data exported to {file_export_name}.csv")

        except json.decoder.JSONDecodeError:
            print(f"Failed to parse comments_submission_ids bridge")
            print(f"Adding to an error.txt file") 
            
            build_error_text_file(f"{comment_ids}\n", file_export_name)
            
            print(f"Done adding url to {file_export_name}_bridge_errors.txt file.  Continuing on...") 
            
            continue 

        break

    return[item["comment_id"] for item in list_comments_submission_id]


def get_comments(comment_ids:List[str], num_retries:int=1, file_export_name:str="comments_data2") -> None:
    
    comment_string = ','.join(comment_ids)

    for num_retry in range(num_retries+1):

        try:

            api_call = f'https://api.pushshift.io/reddit/search/comment?ids={comment_string}'
            
            time.sleep(2)

            req = r.get(api_call)

            comments = req.json()['data'] 

            build_json_file(comments, file_export_name)

            print(f"Data exported to {file_export_name}.csv")

        except json.decoder.JSONDecodeError:
            print(f"Failed to parse: {api_call}")
            print(f"Adding url to {file_export_name}_errors.txt file") 

            build_error_text_file(f"{api_call}\n", file_export_name)
            
            print(f"Done adding url to {file_export_name}_errors.txt file.  Continuing on...") 
            continue 

        break


def main() -> None:
    #edate = date.today()
    start_date = date(2020, 4, 1)
    end_date = date(2020, 4, 2)
    timestamps = make_date_range(start_date, end_date)
    
    queries=['covid', 'coronavirus','sars-cov-2']
    subreddits=['Canada', 'CanadaPolitics', 'CanadaCoronavirus', 
                'Vancouver', 'Edmonton', 'Winnipeg', 
                'Montreal', 'Ottawa', 'Saskatoon', 
                'Calgary', 'Toronto', 'Ontario', "onguardforthee"]
    
    for query in queries:
        print(f'Query: {query}')

        for subreddit in subreddits:
            print(f'Subreddit: {subreddit}')

            posts_id = export_submissions(subreddit=subreddit, query=query, timestamps=timestamps) 

            for post_id in posts_id:
                comments_id = get_comments_id_from_submission_id(post_id)

                get_comments(comments_id)


if __name__ == "__main__":
    main()